# 信息的度量

## 1. 自信息量

信源空间中,符号信息出现的概率和其信息量是单调递减的关系

定义: 概率为 $p(x_i)$ 的符号 $x_i$ 的自信息量为

 $$
    I(x_i) = -\log{p(x_i)} \tag{2-2-1}
 $$

自信息量的单位 通常与所用的对数底数有关, 在信息论中:

1. 常用的对数底数是 2, 信息量的单位是 `比特(bit)`

2. 若取自然对数 e, 信息量的单位是 `奈特(nat)`

3. 若取对数底数为 10, 信息量的单位是 `笛特(det)`

$$
1nat = \log_{2}{e} \approx 1.433bit \\
1det = \log_{2}{10} \approx 3.322bit
$$

### 特性

1. $p(x_i) = 1, I(x_i)=0$;
2. $p(x_i) = 0, I(x_i)=\infty$;
3. 非负性
4. 单调递减性 $if: p(x_1) < p(x_2) then: I(x_1) > I(x_2)$
5. 可加性 $if: x_i 和 y_j 相互独立 then: I(x_i, y_j) = I(x_i) + I(y_j)$

## 条件自信息量

$$
I(x_i | y_j) = -\log{p(x_i | y_j)} \tag{2-2-2}
$$

推导公式:

$I(x_i , y_j) = I(x_i | y_j) + I(y_j)$

## 2. 离散 `信源熵（信源的平均不确定度）`, 表征一个信源的杂乱程度

> 信源 X 的`平均自信息量E(I(X))`和`平均不确定度H(X)`之间是有区别的:
>
> 1. 每个符号中包含的信息量(bit)
> 2. 区分信源中的各个符号所需的信息量(bit)

$$
E(I(X)) or H(X)= \sum \limits_{i} p(x_i)I(x_i) = - \sum \limits_{i} p(x_i)\log{p(x_i)} \tag{2-2-3}
$$

> 信源给定 -> 概率空间即定 -> 信源熵即定 ->　不同的信源因为不同的概率空间而有不同的信源熵

二元信源是离散信源的一个特例

## 条件熵

在给定$Y$(即各个$y_j$)条件下, X集合的条件熵$H(X|Y)$定义为:
$$
H(X | Y) = \sum \limits_{ij} p(x_i, y_j)I(x_i | y_j) \tag{2-2-4}
$$

在给定$X$(即各个$x_i$)条件下, Y集合的条件熵$H(Y|X)$定义为:
$$
H(Y | X) = \sum \limits_{ij} p(x_i, y_j)I(y_j | x_i) \tag{2-2-5}
$$

## 联合熵

$$
H(X , Y) = \sum \limits_{ij} p(x_i, y_j)I(x_i , y_j) = - \sum \limits_{ij} p(x_i, y_j) \log{p(x_i, y_j)} \tag{2-2-6}
$$

## 联合熵 和 条件熵 之间的关系

$$
H(X , Y) = H(X) + H(Y | X) = H(Y) + H(X | Y)
$$

## 符号 x 的`先验概率`和`后验概率`

## 互信息

接收者通过信道接收到的信源 X 的信息量, 称为 X 和 Y 的互信息量, 记作 $I(X;Y)$

1. 平均意义上的互信息量
2. 单个符号上的互信息量

$$
I(X;Y) = I(Y;X)
$$

互信息量的范围是 $0 \le I(X;Y) \le H(X)$: 当在通信端中,发送端的符号是X, 接收端的符号是Y, $I(X;Y)表示接收端收到$Y$后所能获得的关于$X$的信息量$ 若干扰很大, 那么 $I(X;Y)=0$, 若没有干扰, 那么 $I(X;Y)=H(X)$

$$
I(x_i ; y_j) = \log{\frac{p(x_i|y_j)}{p(x_i)}} \tag{2-2-7}
$$

## 平均互信息量

$$
I(X;Y) = \sum_{i,j}p(x_i,y_j)\log{\frac{p(x_i|y_j)}{p(x_i)}}
$$

## 互信息 $I(x_i ; y_j)$ 在 X 集合上的统计平均值

$$
I(X ; y_j) = \sum \limits_{i}p(x_i | y_j) I(x_i ; y_j)
$$

## 相对熵

## 熵的性质

1. 非负性
2. 确定性
3. 对称性
4. 香农辅助定理
   $$
   对任意n维概率矢量P=(p_1,p_2,\dots,p_n), Q=(q_1,q_2,\dots,q_n),总有: \\
   H(p_1,p_2,\dots,p_n) = - \sum_i^n p_i \log{p_i} \le = \sum_i^n p_i \log{q_i}
   $$
5. 最大熵定理
6. 条件熵小于无条件熵
7. 扩展性
8. 可加性
9. 递增性